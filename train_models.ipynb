{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train models on the ToldBR and HateBR datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/hatespeech-pln/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "from hate_nlp.dataset import ToldBRDataset, HateBRDataset\n",
    "from hate_nlp.preprocess import PreprocessToldBR\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def train_model_cv(model_name, dataset, folds=2):\n",
    "    df = dataset.get_dataframe()\n",
    "    skf = StratifiedKFold(n_splits=folds)\n",
    "    X, y = df['text'], df['labels']\n",
    "\n",
    "    results = []\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "      model = build_model(model_name)\n",
    "      df_train = df.iloc[train_index]\n",
    "      df_test = df.iloc[test_index]\n",
    "      model.train_model(df_train)\n",
    "      result, model_outputs, wrong_predictions = model.eval_model(df_test, acc=sklearn.metrics.accuracy_score)\n",
    "      results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def build_model(model_name):\n",
    "    if model_name == 'distilbert-toldbr':\n",
    "      model = ClassificationModel(\n",
    "            \"distilbert\", \"distilbert-base-multilingual-cased\",\n",
    "            args={\n",
    "              'num_train_epochs': 1,\n",
    "              'evaluate_during_training': False,\n",
    "              'overwrite_output_dir': True,\n",
    "              'do_lower_case': False,\n",
    "              'save_steps': 100000,\n",
    "              'no_cache': True,\n",
    "              'n_gpu': 1,\n",
    "              'train_batch_size': 32,\n",
    "              'max_seq_len': 512,\n",
    "              'silent': True,\n",
    "              \"reprocess_input_data\": True,\n",
    "            },\n",
    "          )\n",
    "    else:\n",
    "      raise NotImplementedError\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use preprocessing method as used in ToldBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toldbr_dataset = ToldBRDataset('/home/jose/Programas/HateSpeech-NLP/data/raw/ToLD-BR.csv', PreprocessToldBR())\n",
    "hatebr_dataset = HateBRDataset('/home/jose/Programas/HateSpeech-NLP/data/raw/HateBR.csv', PreprocessToldBR())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Distilbert as in the ToldBR classification examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hatespeech-pln/lib/python3.12/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_toldbr \u001b[38;5;241m=\u001b[39m train_model_cv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-toldbr\u001b[39m\u001b[38;5;124m'\u001b[39m, toldbr_dataset, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m, in \u001b[0;36mtrain_model_cv\u001b[0;34m(model_name, dataset, folds)\u001b[0m\n\u001b[1;32m     15\u001b[0m df_train \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[train_index]\n\u001b[1;32m     16\u001b[0m df_test \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[0;32m---> 17\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_model(df_train)\n\u001b[1;32m     18\u001b[0m result, model_outputs, wrong_predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval_model(df_test, acc\u001b[38;5;241m=\u001b[39msklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39maccuracy_score)\n\u001b[1;32m     19\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/hatespeech-pln/lib/python3.12/site-packages/simpletransformers/classification/classification_model.py:617\u001b[0m, in \u001b[0;36mClassificationModel.train_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m         )\n\u001b[1;32m    613\u001b[0m         train_examples \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    614\u001b[0m             train_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    615\u001b[0m             train_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    616\u001b[0m         )\n\u001b[0;32m--> 617\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_cache_examples(\n\u001b[1;32m    618\u001b[0m         train_examples, verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[1;32m    619\u001b[0m     )\n\u001b[1;32m    620\u001b[0m train_sampler \u001b[38;5;241m=\u001b[39m RandomSampler(train_dataset)\n\u001b[1;32m    621\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    622\u001b[0m     train_dataset,\n\u001b[1;32m    623\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mtrain_sampler,\n\u001b[1;32m    624\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    625\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    626\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hatespeech-pln/lib/python3.12/site-packages/simpletransformers/classification/classification_model.py:1824\u001b[0m, in \u001b[0;36mClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m   1823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1824\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ClassificationDataset(\n\u001b[1;32m   1825\u001b[0m         examples,\n\u001b[1;32m   1826\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m   1827\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m   1828\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   1829\u001b[0m         multi_label\u001b[38;5;241m=\u001b[39mmulti_label,\n\u001b[1;32m   1830\u001b[0m         output_mode\u001b[38;5;241m=\u001b[39moutput_mode,\n\u001b[1;32m   1831\u001b[0m         no_cache\u001b[38;5;241m=\u001b[39mno_cache,\n\u001b[1;32m   1832\u001b[0m     )\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/miniconda3/envs/hatespeech-pln/lib/python3.12/site-packages/simpletransformers/classification/classification_utils.py:282\u001b[0m, in \u001b[0;36mClassificationDataset.__init__\u001b[0;34m(self, data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, tokenizer, args, mode, multi_label, output_mode, no_cache):\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m build_classification_dataset(\n\u001b[1;32m    283\u001b[0m         data, tokenizer, args, mode, multi_label, output_mode, no_cache\n\u001b[1;32m    284\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/hatespeech-pln/lib/python3.12/site-packages/simpletransformers/classification/classification_utils.py:249\u001b[0m, in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    243\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    244\u001b[0m             (text_a[i : i \u001b[38;5;241m+\u001b[39m chunksize], \u001b[38;5;28;01mNone\u001b[39;00m, tokenizer, args\u001b[38;5;241m.\u001b[39mmax_seq_length)\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text_a), chunksize)\n\u001b[1;32m    246\u001b[0m         ]\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(args\u001b[38;5;241m.\u001b[39mprocess_count) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m--> 249\u001b[0m         examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    250\u001b[0m             tqdm(\n\u001b[1;32m    251\u001b[0m                 p\u001b[38;5;241m.\u001b[39mimap(preprocess_data_multiprocessing, data),\n\u001b[1;32m    252\u001b[0m                 total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text_a) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m chunksize,\n\u001b[1;32m    253\u001b[0m                 disable\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msilent,\n\u001b[1;32m    254\u001b[0m             )\n\u001b[1;32m    255\u001b[0m         )\n\u001b[1;32m    257\u001b[0m     examples \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    258\u001b[0m         key: torch\u001b[38;5;241m.\u001b[39mcat([example[key] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples])\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    260\u001b[0m     }\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/hatespeech-pln/lib/python3.12/site-packages/tqdm/std.py:1169\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m# If the bar is disabled, then just walk the iterable\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m# (note: keep this check outside the loop for performance)\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[0;32m-> 1169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hatespeech-pln/lib/python3.12/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/miniconda3/envs/hatespeech-pln/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_toldbr = train_model_cv('distilbert-toldbr', toldbr_dataset, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_hatebr = train_model_cv('distilbert-toldbr', hatebr_dataset, 5) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hatespeech-pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
